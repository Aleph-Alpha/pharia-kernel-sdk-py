"""
This module exposes the interfaces for skills to interact with the Pharia Kernel
via the Cognitive System Interface (CSI).
"""

from dataclasses import dataclass, field
from enum import Enum
from typing import Protocol

from .wit.imports import csi
from .wit.imports.csi import (
    ChatParams as WitChatParams,
)
from .wit.imports.csi import (
    ChatResponse as WitChatResponse,
)
from .wit.imports.csi import (
    ChunkParams,
    CompletionRequest,
    DocumentPath,
    IndexPath,
    Language,
    SearchResult,
)
from .wit.imports.csi import (
    Completion as WitCompletion,
)
from .wit.imports.csi import (
    CompletionParams as WitCompletionParams,
)
from .wit.imports.csi import (
    FinishReason as WitFinishReason,
)
from .wit.imports.csi import (
    Message as WitMessage,
)
from .wit.imports.csi import (
    Role as WitRole,
)

__all__ = [
    "ChatParams",
    "ChatResponse",
    "Message",
    "Role",
    "ChunkParams",
    "Completion",
    "CompletionParams",
    "CompletionRequest",
    "Csi",
    "FinishReason",
    "Language",
    "SearchResult",
    "DocumentPath",
    "IndexPath",
]


@dataclass
class CompletionParams(WitCompletionParams):
    """Completion request parameters.

    Attributes:
        max-tokens (int, optional, default None): The maximum tokens that should be inferred.
            Note, the backing implementation may return less tokens due to other stop reasons.
        temperature (float, optional, default None): The randomness with which the next token is selected.
        top-k (int, optional, default None): The number of possible next tokens the model will choose from.
        top-p (float, optional, default None): The probability total of next tokens the model will choose from.
        stop (list(str), optional, default []): A list of sequences that, if encountered, the API will stop generating further tokens.
    """

    max_tokens: int | None = None
    temperature: float | None = None
    top_k: int | None = None
    top_p: float | None = None
    stop: list[str] = field(default_factory=lambda: list())


class FinishReason(str, Enum):
    """The reason the model finished generating.

    Attributes:
        STOP: The model hit a natural stopping point or a provided stop sequence.
        LENGTH: The maximum number of tokens specified in the request was reached.
        CONTENT_FILTER: Content was omitted due to a flag from content filters.
    """

    STOP = "stop"
    LENGTH = "length"
    CONTENT_FILTER = "content_filter"

    @classmethod
    def from_wit(cls, reason: WitFinishReason) -> "FinishReason":
        match reason:
            case WitFinishReason.STOP:
                return FinishReason.STOP
            case WitFinishReason.LENGTH:
                return FinishReason.LENGTH
            case WitFinishReason.CONTENT_FILTER:
                return FinishReason.CONTENT_FILTER


@dataclass
class Completion:
    """The result of a completion, including the text generated as well as
    why the model finished completing.

    text (str, required): The text generated by the model,
    finish-reason : The reason the model finished generating
    """

    text: str
    finish_reason: FinishReason

    @classmethod
    def from_wit(cls, completion: WitCompletion) -> "Completion":
        return cls(
            text=completion.text,
            finish_reason=FinishReason.from_wit(completion.finish_reason),
        )


@dataclass
class ChatParams(WitChatParams):
    """Chat request parameters.

    Attributes:
        max-tokens (int, optional, default None):  The maximum tokens that should be inferred.
            Note, the backing implementation may return less tokens due to other stop reasons.
        temperature (float, optional, default None): The randomness with which the next token is selected.
        top-p (float, optional, default None): The probability total of next tokens the model will choose from.
    """

    max_tokens: int | None = None
    temperature: float | None = None
    top_p: float | None = None


class Role(str, Enum):
    """A role used for a message in a chat."""

    User = "User"
    Assistant = "Assistant"
    System = "System"

    @property
    def wit(self) -> WitRole:
        match self:
            case Role.User:
                return WitRole.USER
            case Role.Assistant:
                return WitRole.ASSISTANT
            case Role.System:
                return WitRole.SYSTEM

    @classmethod
    def from_wit(cls, role: WitRole) -> "Role":
        match role:
            case WitRole.USER:
                return Role.User
            case WitRole.ASSISTANT:
                return Role.Assistant
            case WitRole.SYSTEM:
                return Role.System


@dataclass
class Message:
    """Describes a message in a chat.

    Parameters:
        role (Role, required): The role of the message.
        content (str, required): The content of the message.
    """

    role: Role
    content: str

    @property
    def wit(self) -> WitMessage:
        return WitMessage(role=self.role.wit, content=self.content)

    @classmethod
    def from_wit(cls, msg: WitMessage) -> "Message":
        return cls(role=Role.from_wit(msg.role), content=msg.content)

    @classmethod
    def user(cls, content: str) -> "Message":
        return cls(role=Role.User, content=content)

    @classmethod
    def assistant(cls, content: str) -> "Message":
        return cls(role=Role.Assistant, content=content)

    @classmethod
    def system(cls, content: str) -> "Message":
        return cls(role=Role.System, content=content)


@dataclass
class ChatResponse:
    """The result of a chat response.

    Attributes:
        message (Message): The generated message.
        finish_reason (FinishReason): Why the model finished completing.
    """

    message: Message
    finish_reason: FinishReason

    @classmethod
    def from_wit(cls, res: WitChatResponse) -> "ChatResponse":
        return cls(
            message=Message.from_wit(res.message),
            finish_reason=FinishReason.from_wit(res.finish_reason),
        )

    @classmethod
    def from_dict(cls, body: dict) -> "ChatResponse":
        return cls(
            message=Message(**body["message"]),
            finish_reason=FinishReason(body["finish_reason"]),
        )


class Csi(Protocol):
    def complete(self, model: str, prompt: str, params: CompletionParams) -> Completion:
        """Generates completions given a prompt.

        Parameters:
            model (str, required): Name of model to use.
            prompt (str, required): The text to be completed.
            params (CompletionParams, required): Parameters for the requested completion.

        Examples:
            >>> prompt = f'''<|begin_of_text|><|start_header_id|>system<|end_header_id|>

            You are a poet who strictly speaks in haikus.<|eot_id|><|start_header_id|>user<|end_header_id|>

            {input.root}<|eot_id|><|start_header_id|>assistant<|end_header_id|>'''
            >>> params = CompletionParams(max_tokens=64)
            >>> completion = csi.complete("llama-3.1-8b-instruct", prompt, params)
        """

    def chunk(self, text: str, params: ChunkParams) -> list[str]:
        """Chunks a text into chunks according to params.

        Parameters:
            text (str, required): text to be chunked
            params (ChunkParams, required): parameter used for chunking, e.g. maximal number of tokens
        """

    def chat(
        self, model: str, messages: list[Message], params: ChatParams
    ) -> ChatResponse:
        """Chat with a model.

        Parameters:
            model (str, required): Name of model to use.
            messages (list[Message], required): List of messages, alternating between messages from user and system
            params (ChatParams, required): parameters used for the chat

        Examples:
            >>> input = "oat milk"
            >>> msg = Message.user(f"You are a poet who strictly speaks in haikus.\n\n{input}")
            >>> model = "llama-3.1-8b-instruct"
            >>> chat_response = csi.chat(model, [msg], ChatParams(max_tokens=64))
        """

    def select_language(self, text: str, languages: list[Language]) -> Language | None:
        """Select the detected language for the provided input based on the list of possible languages.
            If no language matches, None is returned.

        Parameters:
            text (str, required): text input
            languages (list[Language], required): All languages that should be considered during detection.

        Examples:
            >>> text = "Ich spreche Deutsch nur ein bisschen."
            >>> languages = [Language.ENG, Language.DEU]
            >>> result = csi.select_language(text, languages)
        """

    def complete_all(self, requests: list[CompletionRequest]) -> list[Completion]:
        """Generates several completions potentially in parallel. Returns as soon as all completions are ready.

        Parameters:
            requests (list[CompletionRequest], required): list of completion requests.

        Examples:
            >>> params = CompletionParams(max_tokens=64)
            >>> request_1 = CompletionRequest(model, "Say hello to Alice", params)
            >>> request_2 = CompletionRequest(model, "Say hello to Bob", params)
            >>> result = csi.complete_all([request_1, request_2])
            >>> len(result) # 2
            >>> "Alice" in result[0].text # True
            >>> "Bob" in result[1].text # True
        """

    def search(
        self,
        index_path: IndexPath,
        query: str,
        max_results: int,
        min_score: float | None,
    ) -> list[SearchResult]:
        """ """


class WasiCsi(Csi):
    def complete(self, model: str, prompt: str, params: CompletionParams) -> Completion:
        completion = csi.complete(model, prompt, params)
        return Completion.from_wit(completion)

    def chunk(self, text: str, params: ChunkParams) -> list[str]:
        return csi.chunk(text, params)

    def chat(
        self, model: str, messages: list[Message], params: ChatParams
    ) -> ChatResponse:
        response = csi.chat(model, [m.wit for m in messages], params)
        return ChatResponse.from_wit(response)

    def select_language(self, text: str, languages: list[Language]) -> Language | None:
        return csi.select_language(text, languages)

    def complete_all(self, requests: list[CompletionRequest]) -> list[Completion]:
        completions = csi.complete_all(requests)
        return [Completion.from_wit(completion) for completion in completions]

    def search(
        self,
        index_path: IndexPath,
        query: str,
        max_results: int,
        min_score: float | None = None,
    ) -> list[SearchResult]:
        return csi.search(index_path, query, max_results, min_score)
