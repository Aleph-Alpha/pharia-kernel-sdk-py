interface inference {
    /// Better understand the source of a completion, specifically on how much each section of a prompt impacts each token of the completion.
    explain: func(request: list<explanation-request>) -> list<list<text-score>>;

    /// A score for a text segment.
    record text-score {
        /// The start index of the text segment w.r.t. to characters in the prompt.
        start: u32,
        /// The length of the text segment w.r.t. to characters in the prompt.
        length: u32,
        /// The score of the text segment, higher means more relevant.
        score: f64,
    }

    /// At which granularity should the target be explained in terms of the prompt.
    /// If you choose, for example, [`granularity.sentence`] then we report the importance score of each
    /// sentence in the prompt towards generating the target output.
    /// The default is [`granularity.auto`] which means we will try to find the granularity that
    /// brings you closest to around 30 explanations. For large prompts, this would likely
    /// be sentences. For short prompts this might be individual words or even tokens.
    enum granularity {
        /// Let the system decide which granularity is most suitable for the given input.
        auto,
        word,
        sentence,
        paragraph,
    }

    record explanation-request {
        /// The prompt that typically was the input of a previous completion request
        prompt: string,
        /// The target string that should be explained. The influence of individual parts
        /// of the prompt for generating this target string will be indicated in the response.
        target: string,
        /// The model to use for the explanation.
        model: string,
        /// The granularity of the explanation.
        granularity: granularity,
    }

    /// The reason the model finished generating
    enum finish-reason {
        /// The model hit a natural stopping point or a provided stop sequence
        stop,
        /// The maximum number of tokens specified in the request was reached
        length,
        /// Content was omitted due to a flag from content filters
        content-filter,
        /// The model called a tool.
        tool-calls,
    }

    record logprob {
        token: list<u8>,
        logprob: f64,
    }

    record distribution {
        /// Logarithmic probability of the token returned in the completion
        sampled: logprob,
        /// Logarithmic probabilities of the most probable tokens, filled if user has set
        /// variant `logprobs` to `top` in chat or completion request.
        top: list<logprob>,
    }

    record token-usage {
        /// Number of tokens in the prompt
        prompt: u32,
        /// Number of tokens in the generated completion
        completion: u32,
    }

    /// The result of a completion, including the text generated as well as
    /// why the model finished completing.
    record completion {
        /// The text generated by the model
        text: string,
        /// The reason the model finished generating
        finish-reason: finish-reason,
        /// Contains the logprobs for the sampled and top n tokens, given that
        /// `completion-request.params.logprobs` has been set to `sampled` or `top`.
        logprobs: list<distribution>,
        /// Usage statistics for the completion request.
        usage: token-usage,
    }

    variant logprobs {
        /// Do not return any logprobs
        no,
        /// Return only the logprob of the tokens which have actually been sampled into the completion.
        sampled,
        /// Request between 0 and 20 tokens
        top(u8),
    }

    /// Completion request parameters
    record completion-request {
        model: string,
        prompt: string,
        params: completion-params
    }

    complete: func(requests: list<completion-request>) -> list<completion>;

    /// Completion request parameters
    record completion-params {
        /// The maximum tokens that should be inferred.
        ///
        /// Note: the backing implementation may return less tokens due to
        /// other stop reasons.
        max-tokens: option<u32>,
        /// The randomness with which the next token is selected.
        temperature: option<f64>,
        /// The number of possible next tokens the model will choose from.
        top-k: option<u32>,
        /// The probability total of next tokens the model will choose from.
        top-p: option<f64>,
        /// A list of sequences that, if encountered, the API will stop generating further tokens.
        stop: list<string>,
        /// Whether to include special tokens like `<|eot_id|>` in the completion
        return-special-tokens: bool,
        /// When specified, this number will decrease (or increase) the probability of repeating
        /// tokens that were mentioned prior in the completion. The penalty is cumulative. The more
        /// a token is mentioned in the completion, the more its probability will decrease.
        /// A negative value will increase the likelihood of repeating tokens.
        frequency-penalty: option<f64>,
        /// The presence penalty reduces the probability of generating tokens that are already
        /// present in the generated text respectively prompt. Presence penalty is independent of the
        /// number of occurrences. Increase the value to reduce the probability of repeating text.
        presence-penalty: option<f64>,
        /// Use this to control the logarithmic probabilities you want to have returned. This is useful
        /// to figure out how likely it had been that this specific token had been sampled.
        logprobs: logprobs,
        /// Echo the prompt in the completion. This may be especially helpful when log_probs is set
        /// to return logprobs for the prompt.
        echo: bool,
    }

    /// A chunk of a completion returned by a completion stream.
    record completion-append {
        /// A chunk of the completion text.
        text: string,
        /// Corresponding log probabilities for each token in the completion.
        logprobs: list<distribution>,
    }

    /// An event emitted by a completion stream.
    variant completion-event {
        /// A chunk of a completion returned by a completion stream.
        append(completion-append),
        /// The reason the completion stream stopped.
        end(finish-reason),
        /// The usage generated by the completion stream.
        usage(token-usage),
    }

    /// Allows for streaming completion tokens as they are generated.
    resource completion-stream {
        /// Creates a new completion-stream resource for a given completion-request.
        constructor(init: completion-request);
        /// Returns the next completion-event from the completion-stream.
        /// Will return None if the stream has finished.
        next: func() -> option<completion-event>;
    }

    record assistant-message {
        content: option<string>,
        tool-calls: option<list<tool-call>>,
    }

    record tool-message {
        content: string,
        tool-call-id: string,
    }

    record other-message {
        role: string,
        content: string,
    }

    variant message {
        assistant(assistant-message),
        tool(tool-message),
        other(other-message),
    }

    /// A tool call as requested by the model.
    record tool-call {
        /// The ID of the tool call.
        id: string,
        /// The name of the function to call.
        name: string,
        /// The arguments to call the function with, as generated by the model in JSON format.
        /// Note that the model does not always generate valid JSON, and may hallucinate parameters not
        /// defined by your function schema.
        /// Validate the arguments in your code before calling your function.
        arguments: string,
    }

    record function {
        /// The name of the function to be called. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
        name: string,
        /// A description of what the function does, used by the model to choose when and how to call the function.
        description: option<string>,
        /// The parameters the functions accepts, described as a JSON Schema object.
        /// See the [guide](https://platform.openai.com/docs/guides/function-calling?api-mode=responses) for examples,
        /// and the [JSON Schema reference](https://json-schema.org/understanding-json-schema/reference) for documentation about the format.
        /// Omitting parameters defines a function with an empty parameter list.
        parameters: option<list<u8>>,
        /// Whether to enable strict schema adherence when generating the function call.
        /// If set to true, the model will follow the exact schema defined in the parameters field.
        /// Only a subset of JSON Schema is supported when strict is true.
        /// Learn more about Structured Outputs in the function calling guide.
        strict: option<bool>,
    }

    variant tool-choice {
        /// None means the model will not call any tool and instead generates a message.
        none,
        /// Auto means the model can pick between generating a message or calling one or more tools.
        auto,
        /// Required means the model must call one or more tools.
        required,
        /// Specifying a particular tool forces the model to call that tool.
        named(string),
    }

    record chat-params {
        /// The maximum number of tokens that can be generated in the chat completion. This value can be used to control costs for text generated via API.
        /// This value is now deprecated by OpenAI in favor of max_completion_tokens, and is not compatible with OpenAI o-series models.
        /// Whether to set `max-tokens` or `max-completion-tokens` is an inference provider specific decision.
        /// While some inference providers like GitHub models and the Aleph Alpha inference expect the user to set `max-tokens`,
        /// OpenAI deprecated it in favor of `max-completion-tokens`. For OpenAI reasoning models, settings `max-tokens` raises an error.
        max-tokens: option<u32>,
        /// An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.
        /// Only supported by distinct inference providers (e.g. OpenAI).
        max-completion-tokens: option<u32>,
        /// The randomness with which the next token is selected.
        temperature: option<f64>,
        /// The probability total of next tokens the model will choose from.
        top-p: option<f64>,
        /// When specified, this number will decrease (or increase) the probability of repeating
        /// tokens that were mentioned prior in the completion. The penalty is cumulative. The more
        /// a token is mentioned in the completion, the more its probability will decrease.
        /// A negative value will increase the likelihood of repeating tokens.
        frequency-penalty: option<f64>,
        /// The presence penalty reduces the probability of generating tokens that are already
        /// present in the generated text respectively prompt. Presence penalty is independent of the
        /// number of occurrences. Increase the value to reduce the probability of repeating text.
        presence-penalty: option<f64>,
        /// Use this to control the logarithmic probabilities you want to have returned. This is useful
        /// to figure out how likely it had been that this specific token had been sampled.
        logprobs: logprobs,
        /// A list of tools the model may call. Currently, only functions are supported as a tool.
        /// Use this to provide a list of functions the model may generate JSON inputs for.
        /// A max of 128 functions are supported.
        tools: option<list<function>>,
        // Controls which (if any) tool is called by the model. none means the model will not call any tool
        // and instead generates a message. auto means the model can pick between generating a message or 
        // calling one or more tools. required means the model must call one or more tools.
        // Specifying a particular tool via {"type": "function", "function": {"name": "my_function"}} forces the model to call that tool.
        // none is the default when no tools are present. auto is the default if tools are present.
        tool-choice: option<tool-choice>,
        /// Whether to allow the model to call multiple tools in parallel. Defaults to true if not specified.
        parallel-tool-calls: option<bool>,
        /// An object specifying the format that the model must output.
        /// Setting to { "type": "json_schema", "json_schema": {...} } enables Structured Outputs which ensures the model
        /// will match your supplied JSON schema. Learn more in the Structured Outputs guide.
        /// Setting to { "type": "json_object" } enables the older JSON mode, which ensures the message the model generates
        /// is valid JSON. Using json_schema is preferred for models that support it.
        response-format: option<response-format>,
        /// Only available for reasoning models (o-series for OpenAI).
        /// Constrains effort on reasoning for reasoning models.
        /// Currently supported values are low, medium, and high.
        /// Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.
        reasoning-effort: option<reasoning-effort>,
    }

    enum reasoning-effort {
        minimal,
        low,
        medium,
        high,
    }

    variant response-format {
        /// The type of response format being defined: `text`
        text,
        /// The type of response format being defined: `json_object`
        json-object,
        /// The type of response format being defined: `json_schema`
        json-schema(json-schema),
    }

    record json-schema {
        /// A description of what the response format is for, used by the model to determine how to respond in the format.
        description: option<string>,
        /// The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
        name: string,
        /// The schema for the response format, described as a JSON Schema object.
        schema: option<list<u8>>,
        /// Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the `schema` field. Only a subset of JSON Schema is supported when `strict` is `true`. To learn more, read the [Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs).
        strict: option<bool>,
    }

    /// The result of a chat response, including the message generated as well as
    /// why the model finished completing.
    record chat-response {
        /// The message generated by the model
        message: assistant-message,
        /// The reason the model finished generating
        finish-reason: finish-reason,
        /// Contains the logprobs for the sampled and top n tokens, given that
        /// `chat-request.params.logprobs` has been set to `sampled` or `top`.
        logprobs: list<distribution>,
        /// Usage statistics for the chat request.
        usage: token-usage,
    }

    record chat-request {
        model: string,
        messages: list<message>,
        params: chat-params,
    }

    chat: func(requests: list<chat-request>) -> list<chat-response>;

    /// A chunk of a message generated by the model.
    record message-append {
        /// A chunk of the message content
        content: string,
        /// Corresponding log probabilities for each token in the message content
        logprobs: list<distribution>,
    }

    record tool-call-chunk {
        index: u32,
        /// The ID of the tool call.
        id: option<string>,
        /// The name of the tool to call.
        name: option<string>,
        /// The arguments to call the function with, as generated by the model in JSON format.
        /// Note that the model does not always generate valid JSON, and may hallucinate parameters
        /// not defined by your function schema. Validate the arguments in your code before calling
        /// your function.
        arguments: option<string>,
    }

    /// An event emitted by the chat-stream resource.
    variant chat-event {
        /// The start of a new message. It includes the role of the message.
        message-begin(string),
        /// A chunk of a message generated by the model.
        message-append(message-append),
        /// The end of a message. It includes the reason for the message end.
        message-end(finish-reason),
        /// The usage from the generated message
        usage(token-usage),
        /// A tool call chunk.
        tool-call(list<tool-call-chunk>),
    }

    resource chat-stream {
        /// Creates a new chat-stream resource for a given chat-request.
        constructor(init: chat-request);
        /// Returns the next chat-event from the chat-stream.
        /// Will return None if the stream has finished.
        next: func() -> option<chat-event>;
    }
}